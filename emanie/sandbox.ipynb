{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/onyxia/work/aml_project/data/raw_data/X_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Chemin vers le fichier texte\n",
    "fichier_texte = \"/home/onyxia/work/aml_project/molprop_prediction/results/grid_search_res.txt\"\n",
    "\n",
    "# Lire le fichier texte en DataFrame\n",
    "df = pd.read_csv(fichier_texte, delimiter=',')  # Vous pouvez ajuster le d√©limiteur selon le format de votre fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden_dim</th>\n",
       "      <th>lr</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>epochs</th>\n",
       "      <th>test_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64</td>\n",
       "      <td>0.001</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>1.508140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>0.010</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>1.646568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64</td>\n",
       "      <td>0.100</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>8.701841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>128</td>\n",
       "      <td>0.001</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>1.309486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>128</td>\n",
       "      <td>0.010</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>2.219313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>128</td>\n",
       "      <td>0.100</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>46.093396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>256</td>\n",
       "      <td>0.001</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>1.356279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>256</td>\n",
       "      <td>0.010</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>2.187661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>256</td>\n",
       "      <td>0.100</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>179.085879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>64</td>\n",
       "      <td>0.001</td>\n",
       "      <td>32</td>\n",
       "      <td>20</td>\n",
       "      <td>1.466928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>64</td>\n",
       "      <td>0.010</td>\n",
       "      <td>32</td>\n",
       "      <td>20</td>\n",
       "      <td>1.571826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>64</td>\n",
       "      <td>0.100</td>\n",
       "      <td>32</td>\n",
       "      <td>20</td>\n",
       "      <td>46.091048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>128</td>\n",
       "      <td>0.001</td>\n",
       "      <td>32</td>\n",
       "      <td>20</td>\n",
       "      <td>1.302890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>128</td>\n",
       "      <td>0.010</td>\n",
       "      <td>32</td>\n",
       "      <td>20</td>\n",
       "      <td>2.572956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>128</td>\n",
       "      <td>0.100</td>\n",
       "      <td>32</td>\n",
       "      <td>20</td>\n",
       "      <td>24.428440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>256</td>\n",
       "      <td>0.001</td>\n",
       "      <td>32</td>\n",
       "      <td>20</td>\n",
       "      <td>1.387815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>256</td>\n",
       "      <td>0.010</td>\n",
       "      <td>32</td>\n",
       "      <td>20</td>\n",
       "      <td>2.271563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>256</td>\n",
       "      <td>0.100</td>\n",
       "      <td>32</td>\n",
       "      <td>20</td>\n",
       "      <td>46.110474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>64</td>\n",
       "      <td>0.001</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>1.489061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>64</td>\n",
       "      <td>0.010</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>1.851475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>64</td>\n",
       "      <td>0.100</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>4.941600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>128</td>\n",
       "      <td>0.001</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>1.246506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>128</td>\n",
       "      <td>0.010</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>2.197092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>128</td>\n",
       "      <td>0.100</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>46.101307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>256</td>\n",
       "      <td>0.001</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>1.415062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>256</td>\n",
       "      <td>0.010</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>2.193589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>256</td>\n",
       "      <td>0.100</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>56.366119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     hidden_dim     lr  batch_size  epochs   test_loss\n",
       "0            64  0.001          16      20    1.508140\n",
       "1            64  0.010          16      20    1.646568\n",
       "2            64  0.100          16      20    8.701841\n",
       "3           128  0.001          16      20    1.309486\n",
       "4           128  0.010          16      20    2.219313\n",
       "5           128  0.100          16      20   46.093396\n",
       "6           256  0.001          16      20    1.356279\n",
       "7           256  0.010          16      20    2.187661\n",
       "8           256  0.100          16      20  179.085879\n",
       "54           64  0.001          32      20    1.466928\n",
       "55           64  0.010          32      20    1.571826\n",
       "56           64  0.100          32      20   46.091048\n",
       "57          128  0.001          32      20    1.302890\n",
       "58          128  0.010          32      20    2.572956\n",
       "59          128  0.100          32      20   24.428440\n",
       "60          256  0.001          32      20    1.387815\n",
       "61          256  0.010          32      20    2.271563\n",
       "62          256  0.100          32      20   46.110474\n",
       "108          64  0.001          64      20    1.489061\n",
       "109          64  0.010          64      20    1.851475\n",
       "110          64  0.100          64      20    4.941600\n",
       "111         128  0.001          64      20    1.246506\n",
       "112         128  0.010          64      20    2.197092\n",
       "113         128  0.100          64      20   46.101307\n",
       "114         256  0.001          64      20    1.415062\n",
       "115         256  0.010          64      20    2.193589\n",
       "116         256  0.100          64      20   56.366119"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['epochs'] == 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by='test_loss', ascending=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "epochs\n",
       "20     27\n",
       "30     27\n",
       "40     27\n",
       "50     27\n",
       "80     27\n",
       "100    27\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.epochs.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['y'] = 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>smiles</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4400</td>\n",
       "      <td>CC(C)c1c(-c2nnc(N3CCC(N)CC3)o2)cn2ncnc(Nc3cc(C...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4401</td>\n",
       "      <td>COc1cc2c(Nc3ccc(Cl)c(Cl)c3F)ncnc2cc1OC[C@H]1CN...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4402</td>\n",
       "      <td>CN1CCN(Cc2ccc3c(c2)Cc2c-3n[nH]c2-c2csc(C#CCCO)...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4403</td>\n",
       "      <td>Cc1ccc(C(=O)Nc2cccc(C(F)(F)F)c2C)cc1-c1ccc2nc(...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4404</td>\n",
       "      <td>CSc1ncc2c(n1)CCN(c1cc(C)cc(C(=O)Nc3cccc(C(F)(F...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                             smiles  y\n",
       "0  4400  CC(C)c1c(-c2nnc(N3CCC(N)CC3)o2)cn2ncnc(Nc3cc(C...  0\n",
       "1  4401  COc1cc2c(Nc3ccc(Cl)c(Cl)c3F)ncnc2cc1OC[C@H]1CN...  0\n",
       "2  4402  CN1CCN(Cc2ccc3c(c2)Cc2c-3n[nH]c2-c2csc(C#CCCO)...  0\n",
       "3  4403  Cc1ccc(C(=O)Nc2cccc(C(F)(F)F)c2C)cc1-c1ccc2nc(...  0\n",
       "4  4404  CSc1ncc2c(n1)CCN(c1cc(C)cc(C(=O)Nc3cccc(C(F)(F...  0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"/home/onyxia/work/aml_project/data/raw_data/X_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.preprocess_ter import *\n",
    "from models.GIN import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir_dataset = \"../dataset/\" + task + \"/\" + dataset + \"/\"\n",
    "radius = 1\n",
    "device = torch.device(\"cpu\")\n",
    "atom_dict = defaultdict(lambda: len(atom_dict))\n",
    "bond_dict = defaultdict(lambda: len(bond_dict))\n",
    "fingerprint_dict = defaultdict(lambda: len(fingerprint_dict))\n",
    "edge_dict = defaultdict(lambda: len(edge_dict))\n",
    "\n",
    "with open(\"../data/raw_data/train_merged.txt\", \"r\") as f:\n",
    "    smiles_property = f.readline().strip().split()\n",
    "    data_original = f.read().strip().split(\"\\n\")\n",
    "\n",
    "\"\"\"Exclude the data contains '.' in its smiles.\"\"\"\n",
    "data_original = [data for data in data_original if \".\" not in data.split()[0]]\n",
    "\n",
    "dataset = []\n",
    "\n",
    "for data in data_original:\n",
    "    smiles, property = data.strip().split()\n",
    "\n",
    "    \"\"\"Create each data with the above defined functions.\"\"\"\n",
    "    mol = Chem.AddHs(Chem.MolFromSmiles(smiles))\n",
    "    atoms = create_atoms(mol, atom_dict)\n",
    "    molecular_size = len(atoms)\n",
    "    i_jbond_dict = create_ijbonddict(mol, bond_dict)\n",
    "    fingerprints = extract_fingerprints(\n",
    "        radius, atoms, i_jbond_dict, fingerprint_dict, edge_dict\n",
    "    )\n",
    "    adjacency = Chem.GetAdjacencyMatrix(mol)\n",
    "\n",
    "    \"\"\"Transform the above each data of numpy\n",
    "    to pytorch tensor on a device (i.e., CPU or GPU).\n",
    "    \"\"\"\n",
    "    fingerprints = torch.LongTensor(fingerprints).to(device)\n",
    "    adjacency = torch.FloatTensor(adjacency).to(device)\n",
    "\n",
    "    property = torch.FloatTensor([[float(property)]]).to(device)\n",
    "\n",
    "    dataset.append((fingerprints, adjacency, molecular_size, property))\n",
    "\n",
    "\n",
    "dataset_train = dataset\n",
    "dataset_train, dataset_dev = split_dataset(dataset_train, 0.9)\n",
    "# dataset_test = create_dataset(\"data_test.txt\")\n",
    "\n",
    "# N_fingerprints = len(fingerprint_dict)\n",
    "\n",
    "# dataset_train, dataset_dev, dataset_test, N_fingerprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3990"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 67,  68, 107,   3,  12,  11,   5,   8,  57,  26,  11,   5,   5, 108,\n",
       "         109,   5,  73,  74,  31,  53,  31,  19,   5,  45,  46,  14,  60,  35,\n",
       "          24,   1, 107,   3,  13,  34,   0,   0,  20,  20,  20,  21,  22,  21,\n",
       "          22,  22,  22,  22,  22,  20,  20,  20,  20,  20,  20,  20,  20,  21,\n",
       "          20,  20,  20,  20,  20,  20,  20,  20]),\n",
       " tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [1., 0., 1.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 64,\n",
       " tensor([[6.0070]]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[3989]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "i = 0\n",
    "for i in range(3900):\n",
    "    fingerprints_tensor = torch.Tensor(dataset[i][0])\n",
    "    adjacency_matrix_tensor = torch.Tensor(dataset[i][1])\n",
    "    molecular_size_tensor = torch.Tensor(dataset[i][2])\n",
    "    target_tensor = dataset[i][3]\n",
    "\n",
    "    sample_data = Data(\n",
    "        x=fingerprints_tensor,\n",
    "        edge_index=adjacency_matrix_tensor,\n",
    "        molecular_size=molecular_size_tensor,\n",
    "        y=target_tensor,\n",
    "    )\n",
    "    dataset.append(sample_data)\n",
    "\n",
    "train_dataset, test_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 1 in argument 0, but got tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/aml_project-HWNn8X9G/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/aml_project-HWNn8X9G/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/aml_project-HWNn8X9G/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/aml_project-HWNn8X9G/lib/python3.11/site-packages/torch_geometric/loader/dataloader.py:55\u001b[0m, in \u001b[0;36mCollater.collate_fn\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, OnDiskDataset):\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mmulti_get(batch))\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/aml_project-HWNn8X9G/lib/python3.11/site-packages/torch_geometric/loader/dataloader.py:48\u001b[0m, in \u001b[0;36mCollater.__call__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(elem)(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)))\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, Sequence) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataLoader found invalid type: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(elem)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/aml_project-HWNn8X9G/lib/python3.11/site-packages/torch_geometric/loader/dataloader.py:48\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(elem)(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)))\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, Sequence) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)]\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataLoader found invalid type: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(elem)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/aml_project-HWNn8X9G/lib/python3.11/site-packages/torch_geometric/loader/dataloader.py:34\u001b[0m, in \u001b[0;36mCollater.__call__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Batch\u001b[38;5;241m.\u001b[39mfrom_data_list(\n\u001b[1;32m     29\u001b[0m         batch,\n\u001b[1;32m     30\u001b[0m         follow_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_batch,\n\u001b[1;32m     31\u001b[0m         exclude_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexclude_keys,\n\u001b[1;32m     32\u001b[0m     )\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdefault_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, TensorFrame):\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_frame\u001b[38;5;241m.\u001b[39mcat(batch, along\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrow\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/aml_project-HWNn8X9G/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/aml_project-HWNn8X9G/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 119\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/aml_project-HWNn8X9G/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    161\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 1 in argument 0, but got tuple"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torchsummary import summary\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'stores'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     13\u001b[0m total_mae \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Nouvelle variable pour stocker la somme des MAE\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/aml_project-HWNn8X9G/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/aml_project-HWNn8X9G/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/aml_project-HWNn8X9G/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/aml_project-HWNn8X9G/lib/python3.11/site-packages/torch_geometric/loader/dataloader.py:55\u001b[0m, in \u001b[0;36mCollater.collate_fn\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, OnDiskDataset):\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mmulti_get(batch))\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/aml_project-HWNn8X9G/lib/python3.11/site-packages/torch_geometric/loader/dataloader.py:28\u001b[0m, in \u001b[0;36mCollater.__call__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     26\u001b[0m elem \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, BaseData):\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_data_list\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfollow_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexclude_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default_collate(batch)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/aml_project-HWNn8X9G/lib/python3.11/site-packages/torch_geometric/data/batch.py:93\u001b[0m, in \u001b[0;36mBatch.from_data_list\u001b[0;34m(cls, data_list, follow_batch, exclude_keys)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_data_list\u001b[39m(\u001b[38;5;28mcls\u001b[39m, data_list: List[BaseData],\n\u001b[1;32m     83\u001b[0m                    follow_batch: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     84\u001b[0m                    exclude_keys: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     85\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Constructs a :class:`~torch_geometric.data.Batch` object from a\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;124;03m    Python list of :class:`~torch_geometric.data.Data` or\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;124;03m    :class:`~torch_geometric.data.HeteroData` objects.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m    :obj:`follow_batch`.\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03m    Will exclude any keys given in :obj:`exclude_keys`.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m     batch, slice_dict, inc_dict \u001b[38;5;241m=\u001b[39m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincrement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     batch\u001b[38;5;241m.\u001b[39m_num_graphs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_list)\n\u001b[1;32m    103\u001b[0m     batch\u001b[38;5;241m.\u001b[39m_slice_dict \u001b[38;5;241m=\u001b[39m slice_dict\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/aml_project-HWNn8X9G/lib/python3.11/site-packages/torch_geometric/data/collate.py:54\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(cls, data_list, increment, add_batch, follow_batch, exclude_keys)\u001b[0m\n\u001b[1;32m     52\u001b[0m key_to_stores \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m data_list:\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m store \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstores\u001b[49m:\n\u001b[1;32m     55\u001b[0m         key_to_stores[store\u001b[38;5;241m.\u001b[39m_key]\u001b[38;5;241m.\u001b[39mappend(store)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# With this, we iterate over each list of storage objects and recursively\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# collate all its attributes into a unified representation:\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m#   elements as attributes that got incremented need to be decremented\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m#   while separating to obtain original values.\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'stores'"
     ]
    }
   ],
   "source": [
    "hidden_dim = 256\n",
    "input_dim = 1\n",
    "lr = 0.001\n",
    "epochs = 100\n",
    "model = GIN(hidden_dim, input_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.MSELoss()\n",
    "mae = nn.L1Loss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_mae = 0  # Nouvelle variable pour stocker la somme des MAE\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        x, edge_index, batch_data = batch.x, batch.edge_index, batch.batch\n",
    "        output = model(x, edge_index, batch_data)\n",
    "        loss = criterion(output, batch.y.view(-1, 1))\n",
    "        mae_value = mae(output, batch.y.view(-1, 1))  # Calcul de la MAE\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        total_mae += mae_value.item()\n",
    "\n",
    "    average_loss = total_loss / len(train_dataloader)\n",
    "    average_mae = total_mae / len(train_dataloader)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {average_loss}, MAE: {average_mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "\n",
    "class MolecularGraphData(Data):\n",
    "    def __init__(self, fingerprints, adjacency_matrix, molecular_size, target):\n",
    "        super(MolecularGraphData, self).__init__()\n",
    "        self.x = fingerprints  # Node features (fingerprints)\n",
    "        self.edge_index = adjacency_matrix  # Edge connectivity\n",
    "        self.molecular_size = molecular_size  # Size of the molecular graph\n",
    "        self.y = target  # Target property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def onehot_encode(x, features:list):\n",
    "    \"\"\"\n",
    "    Maps input elements x not in features to the last element\n",
    "    \"\"\"\n",
    "    if x not in features: x = features[-1]\n",
    "    binary_encoding = [int(bool_val) for bool_val in list(\n",
    "       map(lambda s: x == s, features))]\n",
    "    return binary_encoding\n",
    "  \n",
    "class onehot_encodings:\n",
    "  ''' encoding class for one hot features'''\n",
    "  def __init__(self, atom_info_func, features):\n",
    "    self.atom_info_func = atom_info_func\n",
    "    self.features = features\n",
    "  \n",
    "  #@property\n",
    "  def onehot_encodings(self, atom):\n",
    "    return onehot_encode(self.atom_info_func(atom), self.features)\n",
    "  \n",
    "  def __len__(self): return len(self.features)\n",
    "\n",
    "def json_to_list(json_file):\n",
    "  with open(json_file, 'r') as f:\n",
    "    input_data = list(json.load(f).values())\n",
    "  print(f\"Loaded json from: {json_file}\")\n",
    "  return input_data\n",
    "\n",
    "def list_to_json(json_file, input_list):\n",
    "  with open(json_file, 'w') as f:\n",
    "    json.dump(input_list, f)\n",
    "  print(f\"Output json saved to: {json_file}\")\n",
    "  \n",
    "from rdkit import Chem\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "\n",
    "class FeaturesArgs:\n",
    "    # encodings information\n",
    "    available_atoms = [\n",
    "        \"C\",\n",
    "        \"N\",\n",
    "        \"O\",\n",
    "        \"S\",\n",
    "        \"F\",\n",
    "        \"Si\",\n",
    "        \"P\",\n",
    "        \"Cl\",\n",
    "        \"Br\",\n",
    "        \"Mg\",\n",
    "        \"Na\",\n",
    "        \"Ca\",\n",
    "        \"Fe\",\n",
    "        \"As\",\n",
    "        \"Al\",\n",
    "        \"I\",\n",
    "        \"B\",\n",
    "        \"V\",\n",
    "        \"K\",\n",
    "        \"Tl\",\n",
    "        \"Yb\",\n",
    "        \"Sb\",\n",
    "        \"Sn\",\n",
    "        \"Ag\",\n",
    "        \"Pd\",\n",
    "        \"Co\",\n",
    "        \"Se\",\n",
    "        \"Ti\",\n",
    "        \"Zn\",\n",
    "        \"Li\",\n",
    "        \"Ge\",\n",
    "        \"Cu\",\n",
    "        \"Au\",\n",
    "        \"Ni\",\n",
    "        \"Cd\",\n",
    "        \"In\",\n",
    "        \"Mn\",\n",
    "        \"Zr\",\n",
    "        \"Cr\",\n",
    "        \"Pt\",\n",
    "        \"Hg\",\n",
    "        \"Pb\",\n",
    "        \"Unknown\",\n",
    "    ]\n",
    "    chirality = [\n",
    "        \"CHI_UNSPECIFIED\",\n",
    "        \"CHI_TETRAHEDRAL_CW\",\n",
    "        \"CHI_TETRAHEDRAL_CCW\",\n",
    "        \"CHI_OTHER\",\n",
    "    ]\n",
    "    num_hydrogens = [0, 1, 2, 3, 4, \"MoreThanFour\"]\n",
    "    n_heavy_atoms = num_hydrogens\n",
    "    formal_charges = [-3, -2, -1, 0, 1, 2, 3, \"Extreme\"]\n",
    "    hybridisation_type = [\"S\", \"SP\", \"SP2\", \"SP3\", \"SP3D\", \"SP3D2\", \"OTHER\"]\n",
    "    # Atoms\n",
    "    # atom encodings\n",
    "    atom_encoding_lambdas = {\n",
    "        \"available_atoms\": onehot_encodings(\n",
    "            lambda atom: str(atom.GetSymbol()), available_atoms\n",
    "        ),\n",
    "        \"chirality_type_enc\": onehot_encodings(\n",
    "            lambda atom: str(atom.GetChiralTag()), chirality\n",
    "        ),\n",
    "        \"hydrogens_implicit\": onehot_encodings(\n",
    "            lambda atom: int(atom.GetTotalNumHs()), num_hydrogens\n",
    "        ),\n",
    "        \"n_heavy_atoms\": onehot_encodings(\n",
    "            lambda atom: int(atom.GetDegree()), n_heavy_atoms\n",
    "        ),\n",
    "        \"formal_charge\": onehot_encodings(\n",
    "            lambda atom: int(atom.GetFormalCharge()), formal_charges\n",
    "        ),\n",
    "        \"hybridisation_type\": onehot_encodings(\n",
    "            lambda atom: str(atom.GetHybridization()), hybridisation_type\n",
    "        ),\n",
    "    }\n",
    "    # atom info\n",
    "    atom_info_lambdas = {\n",
    "        \"is_in_a_ring_enc\": lambda atom: [int(atom.IsInRing())],\n",
    "        \"is_aromatic_enc\": lambda atom: [int(atom.GetIsAromatic())],\n",
    "        \"atomic_mass_scaled\": lambda atom: [float((atom.GetMass() - 10.812) / 116.092)],\n",
    "        \"vdw_radius_scaled\": lambda atom: [\n",
    "            float((Chem.GetPeriodicTable().GetRvdw(atom.GetAtomicNum()) - 1.5) / 0.6)\n",
    "        ],\n",
    "        \"covalent_radius_scaled\": lambda atom: [\n",
    "            float(\n",
    "                (Chem.GetPeriodicTable().GetRcovalent(atom.GetAtomicNum()) - 0.64)\n",
    "                / 0.76\n",
    "            )\n",
    "        ],\n",
    "    }\n",
    "    # compute node feature length\n",
    "    n_node_features = sum(map(len, atom_encoding_lambdas.values()))\n",
    "    n_node_features += len(atom_info_lambdas)\n",
    "\n",
    "    # Bonds encoding info\n",
    "    bond_types = [\n",
    "        Chem.rdchem.BondType.SINGLE,\n",
    "        Chem.rdchem.BondType.DOUBLE,\n",
    "        Chem.rdchem.BondType.TRIPLE,\n",
    "        Chem.rdchem.BondType.AROMATIC,\n",
    "    ]\n",
    "    stereo_types = [\"STEREOZ\", \"STEREOE\", \"STEREOANY\", \"STEREONONE\"]\n",
    "    # bond encodings\n",
    "    bond_encoding_lambdas = {\n",
    "        \"bond_types\": onehot_encodings(lambda bond: bond.GetBondType(), bond_types),\n",
    "        \"stereo_types\": onehot_encodings(\n",
    "            lambda bond: str(bond.GetStereo()), stereo_types\n",
    "        ),\n",
    "    }\n",
    "    # bond quantity\n",
    "    bond_info_lambas = {\n",
    "        \"bond_is_conj_enc\": lambda bond: [int(bond.GetIsConjugated())],\n",
    "        \"bond_is_in_ring_enc\": lambda bond: [int(bond.IsInRing())],\n",
    "    }\n",
    "    n_edge_features = sum(map(len, bond_encoding_lambdas.values()))\n",
    "    n_edge_features += len(bond_info_lambas)\n",
    "    #\n",
    "    n_features = n_edge_features + n_node_features\n",
    "    # Molecule\n",
    "    # lambda mol: GraphDescriptors.BalabanJ(mol)\n",
    "\n",
    "\n",
    "class ModelArgs:\n",
    "    available_models = [\"GIN\"]\n",
    "    model = \"GIN\"\n",
    "\n",
    "\n",
    "class TrainArgs:\n",
    "    batch_size = 2**5\n",
    "    lr = 5e-3\n",
    "    weight_decay = 5e-4\n",
    "    epochs = 20\n",
    "    name = \"default-GIN\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model_save_pt = Path(f\"./model/{name}.pth\")\n",
    "\n",
    "\n",
    "class InferArgs:\n",
    "    batch_size = 1\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    name = \"default-GIN\"\n",
    "    output_path = Path(\"./output\")\n",
    "    model_save_pt = Path(f\"./model/{name}.pth\")\n",
    "\n",
    "\n",
    "class DataArgs:\n",
    "    data_source = Path(\"data/kinase_JAK.csv\")\n",
    "    num_workers = 4\n",
    "    device = TrainArgs.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.loader import DataLoader\n",
    "from models.GIN import GIN\n",
    "from scripts.preprocess import MolDataset\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import Data\n",
    "from rdkit import Chem\n",
    "import networkx as nx\n",
    "\n",
    "def load_params(config_path):\n",
    "    with open(config_path, \"r\") as config_file:\n",
    "        params = json.load(config_file)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = pd.read_csv(\n",
    "    \"/Users/emanieluu/Documents/ENSAE/3A/Advanced Machine Learning/aml_project/data/raw_data/train_merged_data.csv\",\n",
    "    index_col=0\n",
    ")\n",
    "train_data, test_data = train_test_split(merged_data, test_size=0.2, random_state=42)\n",
    "train_dataset = MolDataset(train_data)\n",
    "test_dataset = MolDataset(test_data)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import GraphDescriptors\n",
    "from rdkit.Chem.rdmolops import GetAdjacencyMatrix\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# from .util import onehot_encodings\n",
    "\n",
    "def get_atom_features(\n",
    "    atom,\n",
    "    available_atoms: list = FeaturesArgs.available_atoms,\n",
    "    atom_encode_lambdas: dict = FeaturesArgs.atom_encoding_lambdas,\n",
    "    atom_info_lambdas: dict = FeaturesArgs.atom_info_lambdas,\n",
    "    debug=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Takes an RDKit atom object as input and gives a 1d-numpy array of atom features as output.\n",
    "    \"\"\"\n",
    "    if \"hydrogens_implicit\" in atom_encode_lambdas:\n",
    "        available_atoms = [\"H\"] + available_atoms\n",
    "    atom_feature_vector = []\n",
    "    # compute atom features\n",
    "    for name, atom_encoding in atom_encode_lambdas.items():\n",
    "        encoding = atom_encoding.onehot_encodings(atom)\n",
    "        atom_feature_vector += encoding\n",
    "        if debug:\n",
    "            print(f\"atom encoding length ({name}): {len(encoding)}\")\n",
    "        # boolean features\n",
    "    for name, info_func in atom_info_lambdas.items():\n",
    "        atom_feature_vector += info_func(atom)\n",
    "        if debug:\n",
    "            print(f\"atom info ({name}): {info_func(atom)}\")\n",
    "    if debug:\n",
    "        print(f\"full atom feature:{len(atom_feature_vector)}\")\n",
    "    return torch.Tensor(atom_feature_vector)\n",
    "\n",
    "\n",
    "def get_bond_features(\n",
    "    bond,\n",
    "    bond_encoding_lambdas: dict = FeaturesArgs.bond_encoding_lambdas,\n",
    "    bond_info_lambdas: dict = FeaturesArgs.bond_info_lambas,\n",
    "    debug=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Takes an RDKit bond object as input and gives a 1d-numpy array of bond features as output.\n",
    "    \"\"\"\n",
    "    bond_feature_vector = []\n",
    "    # compute bond features\n",
    "    for name, bond_encoding in bond_encoding_lambdas.items():\n",
    "        encoding = bond_encoding.onehot_encodings(bond)\n",
    "        bond_feature_vector += encoding\n",
    "        if debug:\n",
    "            print(f\"bond encoding length ({name}): {len(bond_feature_vector)}\")\n",
    "        # boolean features\n",
    "    for name, info_func in bond_info_lambdas.items():\n",
    "        bond_feature_vector += info_func(bond)\n",
    "        if debug:\n",
    "            print(f\"bond info ({name}): {info_func(bond)}\")\n",
    "        return torch.Tensor(bond_feature_vector)\n",
    "\n",
    "\n",
    "def smile_to_data(smiles, y_val):\n",
    "    \"\"\"smile to pyg Data components\"\"\"\n",
    "    # convert SMILES to RDKit mol object\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    # get feature dimensions\n",
    "    n_nodes = mol.GetNumAtoms()\n",
    "    n_edges = 2 * mol.GetNumBonds()\n",
    "\n",
    "    # construct node feature matrix X of shape (n_nodes, n_node_features)\n",
    "    for n, atom in enumerate(mol.GetAtoms()):\n",
    "        atom_features = get_atom_features(atom)\n",
    "        if n == 0:\n",
    "            X = torch.zeros((n_nodes, len(atom_features)), dtype=torch.float)\n",
    "        X[atom.GetIdx(), :] = atom_features\n",
    "\n",
    "    # construct edge index array E of shape (2, n_edges)\n",
    "    E_ij = torch.stack(\n",
    "        list(\n",
    "            map(\n",
    "                lambda arr: torch.Tensor(arr).to(torch.long),\n",
    "                np.nonzero(GetAdjacencyMatrix(mol)),\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    # construct edge feature array EF of shape (n_edges, n_edge_features)\n",
    "    EF = torch.stack(\n",
    "        [\n",
    "            get_bond_features(mol.GetBondBetweenAtoms(i.item(), j.item()))\n",
    "            for i, j in zip(E_ij[0], E_ij[1])\n",
    "        ]\n",
    "    )\n",
    "    # construct label tensor\n",
    "    y_tensor = torch.tensor(np.array([y_val]), dtype=torch.float)\n",
    "    return X, E_ij, EF, y_tensor\n",
    "\n",
    "\n",
    "def graph_datalist_from_smiles_and_labels(x_smiles, y):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "      x_smiles [list]: SMILES strings\n",
    "      y [list]: numerial labels for the SMILES strings\n",
    "    Outputs:\n",
    "      data_list [list]: torch_geometric.data.Data objects which represent labeled molecular graphs that can readily be used for machine learning\n",
    "\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    for smiles, y_val in zip(x_smiles, y):\n",
    "        X, E, EF, y_tensor = smile_to_data(smiles, y_val)\n",
    "        # construct Pytorch Geometric data object list\n",
    "        data_list.append(Data(x=X, edge_index=E, edge_attr=EF, y=y_tensor))\n",
    "    return data_list\n",
    "\n",
    "\n",
    "def Molecule_pKa_dataloader(\n",
    "    df,\n",
    "    batch_size,\n",
    "    num_workers: int = 1,\n",
    "    shuffle=False,\n",
    "    device=\"cpu\",\n",
    "    processed_dataset_path=\"./data\",\n",
    "):\n",
    "    smile_col, label_col = \"SMILES\", \"measurement_value\"\n",
    "    x_smiles, y = df[smile_col].to_numpy(), df[label_col].to_numpy()\n",
    "\n",
    "    # create list of molecular graph objects from list of SMILES x_smiles and list of labels y\n",
    "    data_list = graph_datalist_from_smiles_and_labels(x_smiles, y)\n",
    "    dataset = Molecule_pKa(processed_dataset_path, data_list)\n",
    "    # create dataloader for training\n",
    "    dataloader = DataLoader(\n",
    "        dataset=data_list,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=lambda x: tuple(x_.to(device) for x_ in default_collate(x)),\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smile_to_data(smiles, y_val):\n",
    "    \"\"\"smile to pyg Data components\"\"\"\n",
    "    # convert SMILES to RDKit mol object\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    # get feature dimensions\n",
    "    n_nodes = mol.GetNumAtoms()\n",
    "    n_edges = 2 * mol.GetNumBonds()\n",
    "\n",
    "    # construct node feature matrix X of shape (n_nodes, n_node_features)\n",
    "    for n, atom in enumerate(mol.GetAtoms()):\n",
    "        atom_features = get_atom_features(atom)\n",
    "        if n == 0:\n",
    "            X = torch.zeros((n_nodes, len(atom_features)), dtype=torch.float)\n",
    "        X[atom.GetIdx(), :] = atom_features\n",
    "\n",
    "    # construct edge index array E of shape (2, n_edges)\n",
    "    E_ij = torch.stack(\n",
    "        list(\n",
    "            map(\n",
    "                lambda arr: torch.Tensor(arr).to(torch.long),\n",
    "                np.nonzero(GetAdjacencyMatrix(mol)),\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    # construct edge feature array EF of shape (n_edges, n_edge_features)\n",
    "    EF = torch.stack(\n",
    "        [\n",
    "            get_bond_features(mol.GetBondBetweenAtoms(i.item(), j.item()))\n",
    "            for i, j in zip(E_ij[0], E_ij[1])\n",
    "        ]\n",
    "    )\n",
    "    # construct label tensor\n",
    "    y_tensor = torch.tensor(np.array([y_val]), dtype=torch.float)\n",
    "    return X, E_ij, EF, y_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data.loc[0].smiles\n",
    "y = train_data.loc[0].y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, E_ij, EF, y_tensor = smile_to_data(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist = graph_datalist_from_smiles_and_labels(train_data[\"smiles\"], train_data[\"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[28, 79], edge_index=[2, 62], edge_attr=[62, 9], y=[1])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml_project-HWNn8X9G",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
